# ===========================================
# Language Tutor Configuration
# ===========================================

# LLM Provider Configuration
# Options: "groq" (cloud, free API) or "local" (GPU required)
# Development: use "groq" (no GPU needed)
# Production: use "local" (requires NVIDIA GPU with 8GB+ VRAM)
LLM_PROVIDER=groq

# Groq API Settings (for LLM_PROVIDER=groq)
# Get your free API key at https://console.groq.com/
GROQ_API_KEY=your-groq-api-key-here
GROQ_MODEL=llama-3.3-70b-versatile
GROQ_BASE_URL=https://api.groq.com/openai/v1

# Local LLM Settings (for LLM_PROVIDER=local)
# Recommended model: Qwen2.5-7B-Instruct-Q4_K_M.gguf
# Download: wget https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q4_K_M.gguf -O models/model.gguf
LOCAL_MODEL_PATH=./models/model.gguf
GPU_LAYERS=-1
CONTEXT_LENGTH=8192

# Embedding Model (works on CPU, no GPU needed)
EMBEDDING_MODEL=all-MiniLM-L6-v2

# ChromaDB Settings (vector database for RAG)
CHROMA_PERSIST_DIRECTORY=./chroma_db

# App Settings
DEBUG=false
CORS_ORIGINS=["http://localhost:3000", "http://127.0.0.1:3000"]

# ===========================================
# Production Settings Example
# ===========================================
# LLM_PROVIDER=local
# LOCAL_MODEL_PATH=./models/model.gguf
# GPU_LAYERS=-1
# CONTEXT_LENGTH=8192
